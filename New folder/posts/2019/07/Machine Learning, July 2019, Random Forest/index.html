<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Mohcine Madkour, Big Data Architectures and more">


        <title>Explained end to end use of machine learning to optimize e-marketing efforts using email marketing campaign data // Mohcine Madkour // Big Data Architectures and more</title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../../../../theme/css/pure.css">
    <link rel="stylesheet" href="../../../../theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
            <hgroup>
                <a href="../../../../author/mohcine-madkour.html" title="See posts by Mohcine Madkour">
                        <img class="avatar" alt="Mohcine Madkour" src="http://www.gravatar.com/avatar/ae08847efc1a85b710f326eb8ee2e907">
                </a>
                <h2 class="article-info">Mohcine Madkour</h2>
                <small class="about-author"></small>
                <h5>Published</h5>
                <p>Sun 07 July 2019</p>
                <a href="/">&larr;Home</a>
            </hgroup>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Explained end to end use of machine learning to optimize e-marketing efforts using email marketing campaign data</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="../../../../tag/machine-learning/">Machine Learning</a>
                                <a class="post-category" href="../../../../tag/july-2019/">July 2019</a>
                                <a class="post-category" href="../../../../tag/random-forest/">Random Forest</a>
                        </p>
                </header>
            </section>
            <p>The data set used in this demo is an email marketing campaign data that includes customer information, described below, as a well as whether the customer responded to the marketing campaign or not. The machine learning task is to design a model that will be able to predict whether a customer will respond to the
marketing campaign based on his/her information. In other words, predict the ‘responded’ target variable described in the data based on all the input variables provided.</p>
<p><img alt="png" src="/images/tabledesc.png"></p>
<h1>1- Data Preparation</h1>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
    <span class="c1"># Data is avavilable in https://github.com/mohcinemadkour/github-open-data-portal</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;marketing_training.csv&#39;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;marketing_test.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">shape</span>
<span class="p">(</span><span class="mi">7414</span><span class="p">,</span> <span class="mi">22</span><span class="p">)</span>

<span class="n">data</span><span class="p">[</span><span class="s1">&#39;responded&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;responded&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;no&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;yes&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>

<span class="n">X_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">21</span><span class="p">]</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">21</span><span class="p">:</span><span class="mi">22</span><span class="p">]</span>
</pre></div>


<h2>Data Engineering (pmonths and pdays)</h2>
<p>For columns pmonths and pdays we have a symbol value (999) which means that client was not previuously contacted. As preprocessing of this info we are going to create an additional column called was_not_previously_contacted that represent the fact that either the client was contacted or not</p>
<div class="highlight"><pre><span></span><span class="c1"># Make sure of the consistency of 999 over pmonths and pdays</span>
<span class="p">(</span><span class="n">X_data</span><span class="p">[</span><span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;pmonths&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">999</span><span class="p">])</span><span class="o">.</span><span class="n">equals</span><span class="p">(</span><span class="n">X_data</span><span class="p">[</span><span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;pdays&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">999</span><span class="p">])</span>

<span class="bp">True</span>





<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;was_not_previously_contacted&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">where</span><span class="p">(</span><span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;pmonths&quot;</span><span class="p">]</span><span class="o">==</span><span class="mi">999</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<h2>Missing values analysis</h2>
<div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">missing_values_table</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate missing values by column, tabulate results</span>

<span class="sd">    Input</span>
<span class="sd">    df: The dataframe</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Total missing values</span>
    <span class="n">mis_val</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="c1"># Percentage of missing values</span>
    <span class="n">mis_val_percent</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">df</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="c1"># Make a table with the results</span>
    <span class="n">mis_val_table</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">mis_val</span><span class="p">,</span> <span class="n">mis_val_percent</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Rename the columns</span>
    <span class="n">mis_val_table_ren_columns</span> <span class="o">=</span> <span class="n">mis_val_table</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="s1">&#39;Missing Values&#39;</span><span class="p">,</span> <span class="mi">1</span> <span class="p">:</span> <span class="s1">&#39;</span><span class="si">% o</span><span class="s1">f Total Values&#39;</span><span class="p">})</span>

    <span class="c1"># Sort the table by percentage of missing descending</span>
    <span class="n">mis_val_table_ren_columns</span> <span class="o">=</span> <span class="n">mis_val_table_ren_columns</span><span class="p">[</span>
        <span class="n">mis_val_table_ren_columns</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span>
    <span class="s1">&#39;</span><span class="si">% o</span><span class="s1">f Total Values&#39;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Print some summary information</span>
    <span class="k">print</span> <span class="p">(</span><span class="s2">&quot;Your selected dataframe has &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="s2">&quot; columns.</span><span class="se">\n</span><span class="s2">&quot;</span>      
        <span class="s2">&quot;There are &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">mis_val_table_ren_columns</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span>
          <span class="s2">&quot; columns that have missing values.&quot;</span><span class="p">)</span>

    <span class="c1"># Return the dataframe with missing information</span>
    <span class="k">return</span> <span class="n">mis_val_table_ren_columns</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">missing_values_table</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Your selected dataframe has 22 columns.
There are 3 columns that have missing values.
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Missing Values</th>
      <th>% of Total Values</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>schooling</th>
      <td>2155</td>
      <td>29.1</td>
    </tr>
    <tr>
      <th>custAge</th>
      <td>1804</td>
      <td>24.3</td>
    </tr>
    <tr>
      <th>day_of_week</th>
      <td>711</td>
      <td>9.6</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="ch">#!pip install missingno</span>
<span class="kn">import</span> <span class="nn">missingno</span> <span class="kn">as</span> <span class="nn">msno</span>
<span class="n">msno</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f782f264650&amp;gt;
</pre></div>


<p><img alt="png" src="/images/output_12_1.png"></p>
<p>I can see that:
- custAge column has missing values with variation in occurrence,
- schooling column are almost filled with missing values with variation in occurrence, and
- day_of_week column has missing values that are sparsely located </p>
<p>From this visualization it is important to know the fact that there is no correlation in missing value locations of the columns with missing values
The bar on the right side of this diagram shows the data completeness for each row. In this dataset, all rows have 18 - 21 valid values and hence 0 - 3 missing values.</p>
<p>Also, missingno.heatmap visualizes the correlation matrix about the locations of missing values in columns.</p>
<div class="highlight"><pre><span></span><span class="n">msno</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7844ff71d0&amp;gt;
</pre></div>


<p><img alt="png" src="/images/output_15_1.png"></p>
<p>I have missing values in custAge which is a numerical variable, and in schooling and day_of_week which are categorical variables</p>
<h2>Missing values removal</h2>
<p><strong>KNN (K Nearest Neighbors)</strong></p>
<p>I use a KNN based machine learning technique for data imputation. In this method, k neighbors are chosen based on some distance measure and their average is used as an imputation estimate. The method requires the selection of the number of nearest neighbors, and a distance metric. KNN can predict both discrete attributes (the most frequent value among the k nearest neighbors) and continuous attributes (the mean among the k nearest neighbors)
The distance metric varies according to the type of data:
1. Continuous Data: The commonly used distance metrics for continuous data are Euclidean, Manhattan and Cosine
2. Categorical Data: Hamming distance is generally used in this case. It takes all the categorical attributes and for each, count one if the value is not the same between two points. The Hamming distance is then equal to the number of attributes for which the value was different.
One of the most attractive features of the KNN algorithm is that it is simple to understand and easy to implement. The non-parametric nature of KNN gives it an edge in certain settings where the data may be highly “unusual”.
One of the obvious drawbacks of the KNN algorithm is that it becomes time-consuming when analyzing large datasets because it searches for similar instances through the entire dataset. Furthermore, the accuracy of KNN can be severely degraded with high-dimensional data because there is little difference between the nearest and farthest neighbor.</p>
<p>##### Code and Documentation Credit : 
 https://towardsdatascience.com/the-use-of-knn-for-missing-values-cf33d935c637</p>
<p>https://towardsdatascience.com/how-to-handle-missing-data-8646b18db0d4</p>
<p>https://gist.github.com/YohanObadia/b310793cd22a4427faaadd9c381a5850</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">hmean</span>
<span class="kn">from</span> <span class="nn">scipy.spatial.distance</span> <span class="kn">import</span> <span class="n">cdist</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">import</span> <span class="nn">numbers</span>


<span class="k">def</span> <span class="nf">weighted_hamming</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Compute weighted hamming distance on categorical variables. For one variable, it is equal to 1 if</span>
<span class="sd">        the values between point A and point B are different, else it is equal the relative frequency of the</span>
<span class="sd">        distribution of the value across the variable. For multiple variables, the harmonic mean is computed</span>
<span class="sd">        up to a constant factor.</span>
<span class="sd">        @params:</span>
<span class="sd">            - data = a pandas data frame of categorical variables</span>
<span class="sd">        @returns:</span>
<span class="sd">            - distance_matrix = a distance matrix with pairwise distance for all attributes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">categories_dist</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">category</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">category</span><span class="p">])</span>
        <span class="n">X_mean</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">X_dot</span> <span class="o">=</span> <span class="n">X_mean</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">transpose</span><span class="p">())</span>
        <span class="n">X_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_dot</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">inplace</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
        <span class="n">categories_dist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">X_np</span><span class="p">)</span>
    <span class="n">categories_dist</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">categories_dist</span><span class="p">)</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">hmean</span><span class="p">(</span><span class="n">categories_dist</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">distances</span>


<span class="k">def</span> <span class="nf">distance_matrix</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">numeric_distance</span> <span class="o">=</span> <span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="n">categorical_distance</span> <span class="o">=</span> <span class="s2">&quot;jaccard&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Compute the pairwise distance attribute by attribute in order to account for different variables type:</span>
<span class="sd">        - Continuous</span>
<span class="sd">        - Categorical</span>
<span class="sd">        For ordinal values, provide a numerical representation taking the order into account.</span>
<span class="sd">        Categorical variables are transformed into a set of binary ones.</span>
<span class="sd">        If both continuous and categorical distance are provided, a Gower-like distance is computed and the numeric</span>
<span class="sd">        variables are all normalized in the process.</span>
<span class="sd">        If there are missing values, the mean is computed for numerical attributes and the mode for categorical ones.</span>

<span class="sd">        Note: If weighted-hamming distance is chosen, the computation time increases a lot since it is not coded in C </span>
<span class="sd">        like other distance metrics provided by scipy.</span>
<span class="sd">        @params:</span>
<span class="sd">            - data                  = pandas dataframe to compute distances on.</span>
<span class="sd">            - numeric_distances     = the metric to apply to continuous attributes.</span>
<span class="sd">                                      &quot;euclidean&quot; and &quot;cityblock&quot; available.</span>
<span class="sd">                                      Default = &quot;euclidean&quot;</span>
<span class="sd">            - categorical_distances = the metric to apply to binary attributes.</span>
<span class="sd">                                      &quot;jaccard&quot;, &quot;hamming&quot;, &quot;weighted-hamming&quot; and &quot;euclidean&quot;</span>
<span class="sd">                                      available. Default = &quot;jaccard&quot;</span>
<span class="sd">        @returns:</span>
<span class="sd">            - the distance matrix</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">possible_continuous_distances</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="s2">&quot;cityblock&quot;</span><span class="p">]</span>
    <span class="n">possible_binary_distances</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="s2">&quot;jaccard&quot;</span><span class="p">,</span> <span class="s2">&quot;hamming&quot;</span><span class="p">,</span> <span class="s2">&quot;weighted-hamming&quot;</span><span class="p">]</span>
    <span class="n">number_of_variables</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">number_of_observations</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Get the type of each attribute (Numeric or categorical)</span>
    <span class="n">is_numeric</span> <span class="o">=</span> <span class="p">[</span><span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">)]</span>
    <span class="n">is_all_numeric</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">is_numeric</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">is_numeric</span><span class="p">)</span>
    <span class="n">is_all_categorical</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">is_numeric</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">is_mixed_type</span> <span class="o">=</span> <span class="ow">not</span> <span class="n">is_all_categorical</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">is_all_numeric</span>

    <span class="c1"># Check the content of the distances parameter</span>
    <span class="k">if</span> <span class="n">numeric_distance</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">possible_continuous_distances</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;The continuous distance &quot;</span> <span class="o">+</span> <span class="n">numeric_distance</span> <span class="o">+</span> <span class="s2">&quot; is not supported.&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">elif</span> <span class="n">categorical_distance</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">possible_binary_distances</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;The binary distance &quot;</span> <span class="o">+</span> <span class="n">categorical_distance</span> <span class="o">+</span> <span class="s2">&quot; is not supported.&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>

    <span class="c1"># Separate the data frame into categorical and numeric attributes and normalize numeric data</span>
    <span class="k">if</span> <span class="n">is_mixed_type</span><span class="p">:</span>
        <span class="n">number_of_numeric_var</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">is_numeric</span><span class="p">)</span>
        <span class="n">number_of_categorical_var</span> <span class="o">=</span> <span class="n">number_of_variables</span> <span class="o">-</span> <span class="n">number_of_numeric_var</span>
        <span class="n">data_numeric</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">is_numeric</span><span class="p">]</span>
        <span class="n">data_numeric</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_numeric</span> <span class="o">-</span> <span class="n">data_numeric</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">data_numeric</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">data_numeric</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
        <span class="n">data_categorical</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">[</span><span class="ow">not</span> <span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">is_numeric</span><span class="p">]]</span>

    <span class="c1"># Replace missing values with column mean for numeric values and mode for categorical ones. With the mode, it</span>
    <span class="c1"># triggers a warning: &quot;SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame&quot;</span>
    <span class="c1"># but the value are properly replaced</span>
    <span class="k">if</span> <span class="n">is_mixed_type</span><span class="p">:</span>
        <span class="n">data_numeric</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data_numeric</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data_categorical</span><span class="p">:</span>
            <span class="n">data_categorical</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data_categorical</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">is_all_numeric</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">data</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">mode</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="c1"># &quot;Dummifies&quot; categorical variables in place</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_all_numeric</span> <span class="ow">and</span> <span class="ow">not</span> <span class="p">(</span><span class="n">categorical_distance</span> <span class="o">==</span> <span class="s1">&#39;hamming&#39;</span> <span class="ow">or</span> <span class="n">categorical_distance</span> <span class="o">==</span> <span class="s1">&#39;weighted-hamming&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_mixed_type</span><span class="p">:</span>
            <span class="n">data_categorical</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data_categorical</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="n">is_all_numeric</span> <span class="ow">and</span> <span class="n">categorical_distance</span> <span class="o">==</span> <span class="s1">&#39;hamming&#39;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">is_mixed_type</span><span class="p">:</span>
            <span class="n">data_categorical</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">pd</span><span class="o">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">data_categorical</span><span class="p">[</span><span class="n">x</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data_categorical</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">([</span><span class="n">pd</span><span class="o">.</span><span class="n">factorize</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">x</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">is_all_numeric</span><span class="p">:</span>
        <span class="n">result_matrix</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">numeric_distance</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">is_all_categorical</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">categorical_distance</span> <span class="o">==</span> <span class="s2">&quot;weighted-hamming&quot;</span><span class="p">:</span>
            <span class="n">result_matrix</span> <span class="o">=</span> <span class="n">weighted_hamming</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result_matrix</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">categorical_distance</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result_numeric</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">data_numeric</span><span class="p">,</span> <span class="n">data_numeric</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">numeric_distance</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">categorical_distance</span> <span class="o">==</span> <span class="s2">&quot;weighted-hamming&quot;</span><span class="p">:</span>
            <span class="n">result_categorical</span> <span class="o">=</span> <span class="n">weighted_hamming</span><span class="p">(</span><span class="n">data_categorical</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result_categorical</span> <span class="o">=</span> <span class="n">cdist</span><span class="p">(</span><span class="n">data_categorical</span><span class="p">,</span> <span class="n">data_categorical</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="n">categorical_distance</span><span class="p">)</span>
        <span class="n">result_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="o">*</span><span class="p">(</span><span class="n">result_numeric</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">number_of_numeric_var</span> <span class="o">+</span> <span class="n">result_categorical</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">*</span>
                               <span class="n">number_of_categorical_var</span><span class="p">)</span> <span class="o">/</span> <span class="n">number_of_variables</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_observations</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">number_of_observations</span><span class="p">)])</span>

    <span class="c1"># Fill the diagonal with NaN values</span>
    <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">result_matrix</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">result_matrix</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">knn_impute</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">attributes</span><span class="p">,</span> <span class="n">k_neighbors</span><span class="p">,</span> <span class="n">aggregation_method</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">numeric_distance</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
               <span class="n">categorical_distance</span><span class="o">=</span><span class="s2">&quot;jaccard&quot;</span><span class="p">,</span> <span class="n">missing_neighbors_threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Replace the missing values within the target variable based on its k nearest neighbors identified with the</span>
<span class="sd">        attributes variables. If more than 50% of its neighbors are also missing values, the value is not modified and</span>
<span class="sd">        remains missing. If there is a problem in the parameters provided, returns None.</span>
<span class="sd">        If to many neighbors also have missing values, leave the missing value of interest unchanged.</span>
<span class="sd">        @params:</span>
<span class="sd">            - target                        = a vector of n values with missing values that you want to impute. The length has</span>
<span class="sd">                                              to be at least n = 3.</span>
<span class="sd">            - attributes                    = a data frame of attributes with n rows to match the target variable</span>
<span class="sd">            - k_neighbors                   = the number of neighbors to look at to impute the missing values. It has to be a</span>
<span class="sd">                                              value between 1 and n.</span>
<span class="sd">            - aggregation_method            = how to aggregate the values from the nearest neighbors (mean, median, mode)</span>
<span class="sd">                                              Default = &quot;mean&quot;</span>
<span class="sd">            - numeric_distances             = the metric to apply to continuous attributes.</span>
<span class="sd">                                              &quot;euclidean&quot; and &quot;cityblock&quot; available.</span>
<span class="sd">                                              Default = &quot;euclidean&quot;</span>
<span class="sd">            - categorical_distances         = the metric to apply to binary attributes.</span>
<span class="sd">                                              &quot;jaccard&quot;, &quot;hamming&quot;, &quot;weighted-hamming&quot; and &quot;euclidean&quot;</span>
<span class="sd">                                              available. Default = &quot;jaccard&quot;</span>
<span class="sd">            - missing_neighbors_threshold   = minimum of neighbors among the k ones that are not also missing to infer</span>
<span class="sd">                                              the correct value. Default = 0.5</span>
<span class="sd">        @returns:</span>
<span class="sd">            target_completed        = the vector of target values with missing value replaced. If there is a problem</span>
<span class="sd">                                      in the parameters, return None</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Get useful variables</span>
    <span class="n">possible_aggregation_method</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;median&quot;</span><span class="p">,</span> <span class="s2">&quot;mode&quot;</span><span class="p">]</span>
    <span class="n">number_observations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">is_target_numeric</span> <span class="o">=</span> <span class="nb">all</span><span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">target</span><span class="p">)</span>

    <span class="c1"># Check for possible errors</span>
    <span class="k">if</span> <span class="n">number_observations</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;Not enough observations.&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">attributes</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">number_observations</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;The number of observations in the attributes variable is not matching the target variable length.&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">k_neighbors</span> <span class="o">&gt;</span> <span class="n">number_observations</span> <span class="ow">or</span> <span class="n">k_neighbors</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;The range of the number of neighbors is incorrect.&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">aggregation_method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">possible_aggregation_method</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;The aggregation method is incorrect.&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">is_target_numeric</span> <span class="ow">and</span> <span class="n">aggregation_method</span> <span class="o">!=</span> <span class="s2">&quot;mode&quot;</span><span class="p">:</span>
        <span class="k">print</span> <span class="s2">&quot;The only method allowed for categorical target variable is the mode.&quot;</span>
        <span class="k">return</span> <span class="bp">None</span>

    <span class="c1"># Make sure the data are in the right format</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">attributes</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">attributes</span><span class="p">)</span>

    <span class="c1"># Get the distance matrix and check whether no error was triggered when computing it</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="n">distance_matrix</span><span class="p">(</span><span class="n">attributes</span><span class="p">,</span> <span class="n">numeric_distance</span><span class="p">,</span> <span class="n">categorical_distance</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">distances</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">None</span>

    <span class="c1"># Get the closest points and compute the correct aggregation method</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]):</span>
        <span class="k">if</span> <span class="n">pd</span><span class="o">.</span><span class="n">isnull</span><span class="p">(</span><span class="n">value</span><span class="p">):</span>
            <span class="n">order</span> <span class="o">=</span> <span class="n">distances</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">argsort</span><span class="p">()[:</span><span class="n">k_neighbors</span><span class="p">]</span>
            <span class="n">closest_to_target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">order</span><span class="p">,</span> <span class="p">:]</span>
            <span class="n">missing_neighbors</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span>  <span class="ow">in</span> <span class="n">closest_to_target</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]]</span>
            <span class="c1"># Compute the right aggregation method if at least more than 50% of the closest neighbors are not missing</span>
            <span class="k">if</span> <span class="nb">sum</span><span class="p">(</span><span class="n">missing_neighbors</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">missing_neighbors_threshold</span> <span class="o">*</span> <span class="n">k_neighbors</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="k">elif</span> <span class="n">aggregation_method</span> <span class="o">==</span> <span class="s2">&quot;mean&quot;</span><span class="p">:</span>
                <span class="n">target</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">closest_to_target</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">closest_to_target</span><span class="p">)))</span>
            <span class="k">elif</span> <span class="n">aggregation_method</span> <span class="o">==</span> <span class="s2">&quot;median&quot;</span><span class="p">:</span>
                <span class="n">target</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ma</span><span class="o">.</span><span class="n">masked_array</span><span class="p">(</span><span class="n">closest_to_target</span><span class="p">,</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">closest_to_target</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">target</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">mode</span><span class="p">(</span><span class="n">closest_to_target</span><span class="p">,</span> <span class="n">nan_policy</span><span class="o">=</span><span class="s1">&#39;omit&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">target</span>
</pre></div>


<p>I started with k_neighbors = 5 but that keep the data with some missing values, then I start incrementing this parametre until I got 100% of imputation of missing values with k_neighbors= 350</p>
<div class="highlight"><pre><span></span><span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;day_of_week&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">knn_impute</span><span class="p">(</span><span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;day_of_week&quot;</span><span class="p">],</span> <span class="n">X_data</span><span class="p">,</span> <span class="n">k_neighbors</span><span class="o">=</span> <span class="mi">350</span><span class="p">,</span> <span class="n">aggregation_method</span><span class="o">=</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">numeric_distance</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
               <span class="n">categorical_distance</span><span class="o">=</span><span class="s2">&quot;jaccard&quot;</span><span class="p">,</span> <span class="n">missing_neighbors_threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;schooling&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">knn_impute</span><span class="p">(</span><span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;schooling&quot;</span><span class="p">],</span> <span class="n">X_data</span><span class="p">,</span> <span class="n">k_neighbors</span><span class="o">=</span><span class="mi">350</span><span class="p">,</span> <span class="n">aggregation_method</span><span class="o">=</span><span class="s2">&quot;mode&quot;</span><span class="p">,</span> <span class="n">numeric_distance</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
               <span class="n">categorical_distance</span><span class="o">=</span><span class="s2">&quot;jaccard&quot;</span><span class="p">,</span> <span class="n">missing_neighbors_threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;custAge&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">knn_impute</span><span class="p">(</span><span class="n">X_data</span><span class="p">[</span><span class="s2">&quot;custAge&quot;</span><span class="p">],</span> <span class="n">X_data</span><span class="p">,</span> <span class="n">k_neighbors</span><span class="o">=</span><span class="mi">350</span><span class="p">,</span> <span class="n">aggregation_method</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">numeric_distance</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span>
               <span class="n">categorical_distance</span><span class="o">=</span><span class="s2">&quot;jaccard&quot;</span><span class="p">,</span> <span class="n">missing_neighbors_threshold</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>
</pre></div>


<p>Checking if there is any missing values after imputation</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">missingno</span> <span class="kn">as</span> <span class="nn">msno</span>
<span class="n">msno</span><span class="o">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>&amp;lt;matplotlib.axes._subplots.AxesSubplot at 0x7f7854a74c50&amp;gt;
</pre></div>


<p><img alt="png" src="/images/output_24_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">missing_values_table</span><span class="p">(</span><span class="n">X_data</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>Your selected dataframe has 22 columns.
There are 0 columns that have missing values.
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Missing Values</th>
      <th>% of Total Values</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table>
</div>

<h2>Numerical data</h2>
<div class="highlight"><pre><span></span><span class="n">X_data</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>&amp;lt;class &#39;pandas.core.frame.DataFrame&#39;&amp;gt;
RangeIndex: 7414 entries, 0 to 7413
Data columns (total 22 columns):
custAge                         7414 non-null float64
profession                      7414 non-null object
marital                         7414 non-null object
schooling                       7414 non-null object
default                         7414 non-null object
housing                         7414 non-null object
loan                            7414 non-null object
contact                         7414 non-null object
month                           7414 non-null object
day_of_week                     7414 non-null object
campaign                        7414 non-null int64
pdays                           7414 non-null int64
previous                        7414 non-null int64
poutcome                        7414 non-null object
emp.var.rate                    7414 non-null float64
cons.price.idx                  7414 non-null float64
cons.conf.idx                   7414 non-null float64
euribor3m                       7414 non-null float64
nr.employed                     7414 non-null float64
pmonths                         7414 non-null float64
pastEmail                       7414 non-null int64
was_not_previously_contacted    7414 non-null int64
dtypes: float64(7), int64(5), object(10)
memory usage: 1.2+ MB
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>custAge</th>
      <th>campaign</th>
      <th>pdays</th>
      <th>previous</th>
      <th>emp.var.rate</th>
      <th>cons.price.idx</th>
      <th>cons.conf.idx</th>
      <th>euribor3m</th>
      <th>nr.employed</th>
      <th>pmonths</th>
      <th>pastEmail</th>
      <th>was_not_previously_contacted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
      <td>7414.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>39.723855</td>
      <td>2.518344</td>
      <td>960.024548</td>
      <td>0.184111</td>
      <td>0.052091</td>
      <td>93.570708</td>
      <td>-40.561316</td>
      <td>3.583141</td>
      <td>5165.224251</td>
      <td>959.797028</td>
      <td>0.361883</td>
      <td>0.960750</td>
    </tr>
    <tr>
      <th>std</th>
      <td>9.243954</td>
      <td>2.695055</td>
      <td>192.845029</td>
      <td>0.516775</td>
      <td>1.568399</td>
      <td>0.578345</td>
      <td>4.649800</td>
      <td>1.744865</td>
      <td>73.108669</td>
      <td>193.969418</td>
      <td>1.261668</td>
      <td>0.194202</td>
    </tr>
    <tr>
      <th>min</th>
      <td>18.000000</td>
      <td>1.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>-3.400000</td>
      <td>92.201000</td>
      <td>-50.800000</td>
      <td>0.634000</td>
      <td>4963.600000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>34.000000</td>
      <td>1.000000</td>
      <td>999.000000</td>
      <td>0.000000</td>
      <td>-1.800000</td>
      <td>93.075000</td>
      <td>-42.700000</td>
      <td>1.334000</td>
      <td>5099.100000</td>
      <td>999.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>38.559547</td>
      <td>2.000000</td>
      <td>999.000000</td>
      <td>0.000000</td>
      <td>1.100000</td>
      <td>93.444000</td>
      <td>-41.800000</td>
      <td>4.857000</td>
      <td>5191.000000</td>
      <td>999.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>44.000000</td>
      <td>3.000000</td>
      <td>999.000000</td>
      <td>0.000000</td>
      <td>1.400000</td>
      <td>93.994000</td>
      <td>-36.400000</td>
      <td>4.961000</td>
      <td>5228.100000</td>
      <td>999.000000</td>
      <td>0.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>94.000000</td>
      <td>40.000000</td>
      <td>999.000000</td>
      <td>6.000000</td>
      <td>1.400000</td>
      <td>94.767000</td>
      <td>-26.900000</td>
      <td>5.045000</td>
      <td>5228.100000</td>
      <td>999.000000</td>
      <td>18.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

<p>It is worth noting that the min, mean, max and 25%,50%75% are properly ordered, therefore the numerical data looks good and there is no need to cast/coerse to numeric</p>
<div class="highlight"><pre><span></span><span class="c1"># Scale numerical features</span>
<span class="n">scaled_num_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">num_data</span><span class="p">),</span> <span class="n">columns</span><span class="o">=</span><span class="n">num_data</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>


<div class="highlight"><pre><span></span>/home/mohcine/Software/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.
  return self.partial_fit(X, y)
/home/mohcine/Software/anaconda2/lib/python2.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.
  return self.fit(X, **fit_params).transform(X)
</pre></div>


<h3>Outlier detection and cleaning</h3>
<div class="highlight"><pre><span></span><span class="n">fig1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">scatter_matrix</span><span class="p">(</span><span class="n">scaled_num_data</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">11</span><span class="p">,</span><span class="mi">11</span><span class="p">),</span> <span class="n">diagonal</span> <span class="o">=</span> <span class="s1">&#39;kde&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>/home/mohcine/Software/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: FutureWarning: pandas.scatter_matrix is deprecated, use pandas.plotting.scatter_matrix instead
  &quot;&quot;&quot;Entry point for launching an IPython kernel.
</pre></div>


<p><img alt="png" src="/images/output_32_1.png"></p>
<p>From the scatter plot I dont see bad outliers in the data</p>
<h2>One-hot encode the categorical data</h2>
<div class="highlight"><pre><span></span><span class="c1"># The OneHotEncoder only works on categorical features. We need first to extract the categorial featuers using boolean mask.</span>
<span class="c1"># Categorical boolean mask</span>
<span class="n">categorical_feature_mask</span> <span class="o">=</span> <span class="n">X_data</span><span class="o">.</span><span class="n">dtypes</span><span class="o">==</span><span class="nb">object</span>

<span class="c1"># filter categorical columns using mask and turn it into a list</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="n">X_data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">categorical_feature_mask</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># find unique labels for each category</span>
<span class="n">X_data</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">nunique</span><span class="p">(),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>profession     12
marital         4
schooling       8
default         3
housing         3
loan            3
contact         2
month          10
day_of_week     5
poutcome        3
dtype: int64
</pre></div>


<p>Pandas get_dummies method get the dummy variables for categorical features.</p>
<div class="highlight"><pre><span></span><span class="c1"># apply One-Hot Encoder on categorical feature columns</span>
<span class="n">cat_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X_data</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">])</span>
<span class="n">cat_data</span><span class="o">.</span><span class="n">tail</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>profession_admin.</th>
      <th>profession_blue-collar</th>
      <th>profession_entrepreneur</th>
      <th>profession_housemaid</th>
      <th>profession_management</th>
      <th>profession_retired</th>
      <th>profession_self-employed</th>
      <th>profession_services</th>
      <th>profession_student</th>
      <th>profession_technician</th>
      <th>...</th>
      <th>month_oct</th>
      <th>month_sep</th>
      <th>day_of_week_fri</th>
      <th>day_of_week_mon</th>
      <th>day_of_week_thu</th>
      <th>day_of_week_tue</th>
      <th>day_of_week_wed</th>
      <th>poutcome_failure</th>
      <th>poutcome_nonexistent</th>
      <th>poutcome_success</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>7409</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7410</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7411</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7412</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7413</th>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>...</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 53 columns</p>
</div>

<p>Notice the new categories that were created. For example, the categorical variable "marital" is split into three categories, 'marital_divorced', 'marital_married', 'marital_single', 'marital_unknown'. Now, that the one hot encoding of the categorical data is done, I need to merge the numerical data from the original data. </p>
<h2>Concatenation</h2>
<div class="highlight"><pre><span></span><span class="n">numerical_cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">X_data</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">categorical_cols</span><span class="p">))</span>
<span class="n">num_data</span> <span class="o">=</span> <span class="n">X_data</span><span class="p">[</span><span class="n">numerical_cols</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># check that the numeric data has been captured accurately</span>
<span class="n">num_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>nr.employed</th>
      <th>custAge</th>
      <th>was_not_previously_contacted</th>
      <th>campaign</th>
      <th>cons.conf.idx</th>
      <th>cons.price.idx</th>
      <th>pastEmail</th>
      <th>pmonths</th>
      <th>euribor3m</th>
      <th>pdays</th>
      <th>emp.var.rate</th>
      <th>previous</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5195.8</td>
      <td>55.000000</td>
      <td>1</td>
      <td>1</td>
      <td>-42.0</td>
      <td>93.200</td>
      <td>0</td>
      <td>999.0</td>
      <td>4.191</td>
      <td>999</td>
      <td>-0.1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5228.1</td>
      <td>37.673913</td>
      <td>1</td>
      <td>1</td>
      <td>-42.7</td>
      <td>93.918</td>
      <td>0</td>
      <td>999.0</td>
      <td>4.960</td>
      <td>999</td>
      <td>1.4</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5191.0</td>
      <td>42.000000</td>
      <td>1</td>
      <td>1</td>
      <td>-36.4</td>
      <td>93.994</td>
      <td>0</td>
      <td>999.0</td>
      <td>4.857</td>
      <td>999</td>
      <td>1.1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5228.1</td>
      <td>55.000000</td>
      <td>1</td>
      <td>2</td>
      <td>-42.7</td>
      <td>93.918</td>
      <td>0</td>
      <td>999.0</td>
      <td>4.962</td>
      <td>999</td>
      <td>1.4</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5099.1</td>
      <td>37.251082</td>
      <td>1</td>
      <td>5</td>
      <td>-46.2</td>
      <td>92.893</td>
      <td>1</td>
      <td>999.0</td>
      <td>1.291</td>
      <td>999</td>
      <td>-1.8</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="c1"># concat numeric and the encoded categorical variables</span>
<span class="n">num_cat_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">num_data</span><span class="p">,</span> <span class="n">cat_data</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># here we do a quick sanity check that the data has been concatenated correctly by checking the dimension of the vectors</span>
<span class="k">print</span><span class="p">(</span><span class="n">cat_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">num_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">num_cat_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(7414, 53)
(7414, 12)
(7414, 65)
</pre></div>


<h2>Spliting to training &amp; testing</h2>
<p>Since I dont have the responded vector in the test set (label), I am going to split the training set to train set + test set</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">num_cat_data</span><span class="p">,</span>
                                                    <span class="n">y_data</span><span class="p">,</span>
                                                    <span class="n">test_size</span><span class="o">=.</span><span class="mi">25</span><span class="p">,</span> 
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># check that the dimensions of our train and test sets are okay</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_test</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>(5560, 65)
(5560, 1)
(1854, 65)
(1854, 1)
</pre></div>


<h3>Pickling</h3>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pickle</span>
<span class="c1"># save the datasets for later use</span>
<span class="n">preprocessed_data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;x_train&#39;</span><span class="p">:</span><span class="n">x_train</span><span class="p">,</span>
    <span class="s1">&#39;y_train&#39;</span><span class="p">:</span><span class="n">y_train</span><span class="p">,</span>
    <span class="s1">&#39;x_test&#39;</span><span class="p">:</span><span class="n">x_test</span><span class="p">,</span>
    <span class="s1">&#39;y_test&#39;</span><span class="p">:</span><span class="n">y_test</span><span class="p">,</span>
<span class="p">}</span>

<span class="c1"># pickle the preprocessed_data</span>
<span class="n">path</span> <span class="o">=</span> <span class="s1">&#39;preprocessed_data_full.pkl&#39;</span>
<span class="n">out</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span>
<span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">preprocessed_data</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>


<h1>Fix class imbalance</h1>
<p>Training a machine learning model on an imbalanced dataset can introduce unique challenges to the learning problem. Imbalanced data typically refers to a classification problem where the number of observations per class is not equally distributed; often you'll have a large amount of data/observations for one class (referred to as the majority class), and much fewer observations for one or more other classes (referred to as the minority classes).
It's worth noting that not all datasets are affected equally by class imbalance. Generally, for easy classification problems in which there's a clear separation in the data, class imbalance doesn't impede on the model's ability to learn effectively. However, datasets that are inherently more difficult to learn from see an amplification in the learning challenge when a class imbalance is introduced.</p>
<div class="highlight"><pre><span></span><span class="ch">#!pip install imblearn</span>
<span class="kn">from</span> <span class="nn">imblearn.over_sampling</span> <span class="kn">import</span> <span class="n">SMOTE</span>
<span class="c1">#Before fitting SMOTE, let us check the y_train values:</span>
<span class="n">y_train</span><span class="p">[</span><span class="s1">&#39;responded&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>0    4969
1     591
Name: responded, dtype: int64
</pre></div>


<div class="highlight"><pre><span></span><span class="mf">591.0</span><span class="o">/</span><span class="mf">4969.0</span><span class="o">*</span><span class="mi">100</span>
</pre></div>


<div class="highlight"><pre><span></span>11.893741195411552
</pre></div>


<p>I have only 11% of responded customers, so there’s some imbalance in the data but it’s not very terrible. </p>
<h2>Oversampling Minor class using SMOTE algorithm on Training data set Only</h2>
<p>I’ll upsample the positive responded using the SMOTE algorithm (Synthetic Minority Oversampling Technique). At a high level, SMOTE creates synthetic observations of the minority class (bad loans) by:</p>
<ul>
<li>Finding the k-nearest-neighbors for minority class observations (finding similar observations)</li>
<li>Randomly choosing one of the k-nearest-neighbors and using it to create a similar, but randomly tweaked, new observation.</li>
</ul>
<p>After upsampling to a class ratio of 1.0, I should have a balanced dataset.</p>
<div class="highlight"><pre><span></span><span class="n">sm</span> <span class="o">=</span> <span class="n">SMOTE</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">ratio</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">fit_sample</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>/home/mohcine/Software/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:761: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().
  y = column_or_1d(y, warn=True)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">bincount</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>array([4969, 4969])
</pre></div>


<h4>Conclusion</h4>
<p>It is worth noticing that by oversampling only on the training data, none of the information in the test data is being used to create synthetic observations. So these results should be generalizable</p>
<h1>2- Exploratory Data Analysis</h1>
<ul>
<li>Numeric features correlation analysis</li>
<li>Creating a simple baseline model (the parsimonious model)</li>
<li>Testing the oversampling on training set </li>
<li>Estimate feature importance by training a random forest regressor</li>
</ul>
<div class="highlight"><pre><span></span><span class="c1"># Run this cell and a very nice matrix will hopefully appear</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span> 
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">num_data</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span><span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;BrBG&#39;</span><span class="p">,</span> <span class="n">annot_kws</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;size&quot;</span><span class="p">:</span> <span class="mi">14</span><span class="p">})</span>
<span class="n">fig</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>/home/mohcine/Software/anaconda2/lib/python2.7/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
  &quot;matplotlib is currently using a non-GUI backend, &quot;
</pre></div>


<p><img alt="png" src="/images/output_58_1.png"></p>
<p>There is many correlation cases between features</p>
<p><strong>Training the classifier</strong></p>
<p>In this particular case, I have chosen to train our classifier using the LogisticRegression module from SciKit Learn, since it's a good starting point for a model, especially when our data is not too large. </p>
<p>To normalize the values, I use the StandardScaler, again from SciKit-Learn.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">x_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
</pre></div>


<p>In the next line I look at the result of scaling. The first table of output shows the statistics for the original values. The second table shows the stats for the scaled values. Column 0 is Age and column 1 is KM.</p>
<div class="highlight"><pre><span></span><span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">x_train_scaled</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>            0        1        2        3        4        5        6        7   \
count  9938.00  9938.00  9938.00  9938.00  9938.00  9938.00  9938.00  9938.00   
mean   5132.88    40.37     0.88     2.29   -40.31    93.48     0.55   881.81   
std      87.36    10.77     0.32     2.26     5.34     0.63     1.55   321.44   
min    4963.60    18.00     0.00     1.00   -50.80    92.20     0.00     0.00   
25%    5076.20    33.03     1.00     1.00   -46.20    92.93     0.00   999.00   
50%    5099.10    38.73     1.00     1.88   -41.80    93.44     0.00   999.00   
75%    5228.10    44.04     1.00     2.87   -36.40    93.99     0.00   999.00   
max    5228.10    94.00     1.00    40.00   -26.90    94.77    18.00   999.00

            8        9    ...          55       56       57       58       59  \
count  9938.00  9938.00   ...     9938.00  9938.00  9938.00  9938.00  9938.00   
mean      2.91   882.51   ...        0.04     0.04     0.17     0.28     0.20   
std       1.89   319.52   ...        0.18     0.19     0.35     0.42     0.37   
min       0.63     0.00   ...        0.00     0.00     0.00     0.00     0.00   
25%       1.25   999.00   ...        0.00     0.00     0.00     0.00     0.00   
50%       1.47   999.00   ...        0.00     0.00     0.00     0.00     0.00   
75%       4.96   999.00   ...        0.00     0.00     0.00     0.69     0.09   
max       5.04   999.00   ...        1.00     1.00     1.00     1.00     1.00

            60       61       62       63       64  
count  9938.00  9938.00  9938.00  9938.00  9938.00  
mean      0.18     0.17     0.11     0.79     0.11  
std       0.36     0.35     0.29     0.40     0.31  
min       0.00     0.00     0.00     0.00     0.00  
25%       0.00     0.00     0.00     1.00     0.00  
50%       0.00     0.00     0.00     1.00     0.00  
75%       0.00     0.00     0.00     1.00     0.00  
max       1.00     1.00     1.00     1.00     1.00

[8 rows x 65 columns]
            0        1        2        3        4        5        6        7   \
count  9938.00  9938.00  9938.00  9938.00  9938.00  9938.00  9938.00  9938.00   
mean     -0.00     0.00     0.00     0.00     0.00     0.00     0.00    -0.00   
std       1.00     1.00     1.00     1.00     1.00     1.00     1.00     1.00   
min      -1.94    -2.08    -2.74    -0.57    -1.97    -2.04    -0.35    -2.74   
25%      -0.65    -0.68     0.36    -0.57    -1.10    -0.89    -0.35     0.36   
50%      -0.39    -0.15     0.36    -0.18    -0.28    -0.06    -0.35     0.36   
75%       1.09     0.34     0.36     0.26     0.73     0.81    -0.35     0.36   
max       1.09     4.98     0.36    16.69     2.51     2.04    11.23     0.36

            8        9    ...          55       56       57       58       59  \
count  9938.00  9938.00   ...     9938.00  9938.00  9938.00  9938.00  9938.00   
mean      0.00     0.00   ...       -0.00     0.00    -0.00     0.00     0.00   
std       1.00     1.00   ...        1.00     1.00     1.00     1.00     1.00   
min      -1.21    -2.76   ...       -0.23    -0.22    -0.49    -0.66    -0.53   
25%      -0.88     0.36   ...       -0.23    -0.22    -0.49    -0.66    -0.53   
50%      -0.76     0.36   ...       -0.23    -0.22    -0.49    -0.66    -0.53   
75%       1.08     0.36   ...       -0.23    -0.22    -0.49     0.98    -0.29   
max       1.13     0.36   ...        5.21     5.04     2.35     1.73     2.18

            60       61       62       63       64  
count  9938.00  9938.00  9938.00  9938.00  9938.00  
mean     -0.00     0.00     0.00     0.00    -0.00  
std       1.00     1.00     1.00     1.00     1.00  
min      -0.51    -0.49    -0.37    -1.96    -0.35  
25%      -0.51    -0.49    -0.37     0.53    -0.35  
50%      -0.51    -0.49    -0.37     0.53    -0.35  
75%      -0.51    -0.49    -0.37     0.53    -0.35  
max       2.27     2.37     3.05     0.53     2.91

[8 rows x 65 columns]
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="c1"># Create a linear model for Logistic Regression</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># we create an instance of Neighbours Classifier and fit the data.</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class=&#39;warn&#39;,
          n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;warn&#39;,
          tol=0.0001, verbose=0, warm_start=False)
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="n">x_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_scaled</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Model Accuracy: {}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">score</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">3</span><span class="p">)))</span>
</pre></div>


<div class="highlight"><pre><span></span>Model Accuracy: 0.802


/home/mohcine/Software/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype uint8, int64, float64 were all converted to float64 by StandardScaler.
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">recall_score</span>
<span class="k">print</span> <span class="s1">&#39;test Results: Accuracy and recall&#39;</span>
<span class="k">print</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test_scaled</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_scaled</span><span class="p">))</span>
<span class="k">print</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Training Results: Accuracy and recall&#39;</span>
<span class="k">print</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train_scaled</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>test Results: Accuracy and recall
0.802049622437972
0.6465863453815262

Training Results: Accuracy and recall
0.7534715234453613
0.6914872207687663
</pre></div>


<h3>Conclusion</h3>
<ul>
<li>
<p>The training results closely match the unseen test data results, which is exactly what I would want to see after putting a model into production.</p>
</li>
<li>
<p>80% accuracy looks good, but not too good classifying non responded customers (Recall). In statistics, recall is the number of correctly predicted “positives” divided by the total number of “positives”.</p>
</li>
</ul>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">clf_rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">clf_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>RandomForestClassifier(bootstrap=True, class_weight=None, criterion=&#39;gini&#39;,
            max_depth=None, max_features=&#39;auto&#39;, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=None,
            oob_score=False, random_state=12, verbose=0, warm_start=False)
</pre></div>


<div class="highlight"><pre><span></span><span class="n">imp</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">clf_rf</span><span class="o">.</span><span class="n">feature_importances_</span> <span class="p">,</span>
        <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Importance&#39;</span><span class="p">]</span> <span class="p">,</span>
        <span class="n">index</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">num_cat_data</span><span class="p">)</span>
    <span class="p">)</span>
<span class="n">imp</span> <span class="o">=</span> <span class="n">imp</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span> <span class="p">[</span> <span class="s1">&#39;Importance&#39;</span> <span class="p">]</span> <span class="p">,</span> <span class="n">ascending</span> <span class="o">=</span> <span class="bp">True</span> <span class="p">)</span>
<span class="c1">#f1 = plt.figure()</span>
<span class="n">f1</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">12</span><span class="p">))</span> 
<span class="n">imp</span><span class="p">[</span><span class="s1">&#39;Importance&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s1">&#39;barh&#39;</span><span class="p">)</span>
<span class="n">f1</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span>/home/mohcine/Software/anaconda2/lib/python2.7/site-packages/matplotlib/figure.py:457: UserWarning: matplotlib is currently using a non-GUI backend, so cannot show the figure
  &quot;matplotlib is currently using a non-GUI backend, &quot;
</pre></div>


<p><img alt="png" src="/images/output_69_1.png"></p>
<h3>Conclusion:</h3>
<p>I was expecting custAge, euribor3m, and schooling_university degree to be among the important features in predition</p>
<h1>3- Building Prediction Model</h1>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># Requirement in order to import xgboost</span>
<span class="n">mingw_path</span> <span class="o">=</span> <span class="s1">&#39;C:</span><span class="se">\\</span><span class="s1">Program Files</span><span class="se">\\</span><span class="s1">mingw-w64</span><span class="se">\\</span><span class="s1">x86_64-6.1.0-posix-seh-rt_v5-rev0</span><span class="se">\\</span><span class="s1">mingw64</span><span class="se">\\</span><span class="s1">bin&#39;</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mingw_path</span> <span class="o">+</span> <span class="s1">&#39;;&#39;</span> <span class="o">+</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span>

<span class="c1">#import all relevant libraries</span>
<span class="kn">import</span> <span class="nn">xgboost</span> <span class="kn">as</span> <span class="nn">xgb</span>
<span class="kn">from</span> <span class="nn">xgboost.sklearn</span> <span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">plot_importance</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span><span class="p">,</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">auc</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">interp</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Define the class weight scale (a hyperparameter) as the ration of negative labels to positive labels.</span>
<span class="c1"># This instructs the classifier to address the class imbalance.</span>
<span class="n">class_weight_scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">*</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">class_weight_scale</span>
</pre></div>


<div class="highlight"><pre><span></span>1.0
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Setting minimal required initial hyperparameters</span>

<span class="n">param</span><span class="o">=</span><span class="p">{</span>
    <span class="s1">&#39;objective&#39;</span><span class="p">:</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span>
    <span class="s1">&#39;nthread&#39;</span><span class="p">:</span><span class="mi">4</span><span class="p">,</span>
    <span class="s1">&#39;scale_pos_weight&#39;</span><span class="p">:</span><span class="n">class_weight_scale</span><span class="p">,</span>
    <span class="s1">&#39;seed&#39;</span> <span class="p">:</span> <span class="mi">1</span>   
<span class="p">}</span>
<span class="n">xgb1</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">xgb1</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="n">param</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span>XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,
       n_jobs=1, nthread=4, objective=&#39;binary:logistic&#39;, random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1.0, seed=1,
       silent=True, subsample=1)
</pre></div>


<div class="highlight"><pre><span></span><span class="c1"># Train initial classifier and analyze performace using K-fold cross-validation </span>
<span class="n">K</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">eval_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">K</span><span class="p">))</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">mean_tpr</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">mean_fpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">roc_aucs_xgb1</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_indices</span><span class="p">,</span> <span class="n">test_indices</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
    <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">test_indices</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>
    <span class="n">class_weight_scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">*</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">print</span> <span class="s1">&#39;class weight scale : {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">class_weight_scale</span><span class="p">)</span>
    <span class="n">xgb1</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s1">&#39;scale_pos_weight&#39;</span> <span class="p">:</span> <span class="n">class_weight_scale</span><span class="p">})</span>
    <span class="n">xgb1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">)</span>
    <span class="n">xgb1_pred_prob</span> <span class="o">=</span> <span class="n">xgb1</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">xgb1_pred_prob</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">mean_tpr</span> <span class="o">+=</span> <span class="n">interp</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">mean_tpr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">roc_aucs_xgb1</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC fold </span><span class="si">%d</span><span class="s1"> (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">))</span>

    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Luck&#39;</span><span class="p">)</span>

<span class="n">mean_tpr</span> <span class="o">/=</span> <span class="n">K</span>
<span class="n">mean_tpr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">mean_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">mean_tpr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">mean_tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Mean ROC (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">mean_auc</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Initial estimator ROC curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/initial_ROC.png&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="k">[1129 1130 1131 ... 9935 9936 9937]</span>
<span class="na">class weight scale : 1.0</span>
<span class="k">[   0    1    2 ... 9935 9936 9937]</span>
<span class="na">class weight scale : 1.0</span>
<span class="k">[   0    1    2 ... 9935 9936 9937]</span>
<span class="na">class weight scale : 1.0</span>
<span class="k">[   0    1    2 ... 9935 9936 9937]</span>
<span class="na">class weight scale : 1.0</span>
<span class="k">[   0    1    2 ... 8942 8943 8944]</span>
<span class="na">class weight scale : 1.0</span>
</pre></div>


<p><img alt="png" src="/images/output_75_1.png"></p>
<h1>Regularization of the Prediction Model</h1>
<div class="highlight"><pre><span></span><span class="c1"># Option to perform hyperparameter optimization. Otherwise loads pre-defined xgb_opt params</span>
<span class="n">optimize</span> <span class="o">=</span> <span class="bp">True</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">x_train</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span>

<span class="k">if</span> <span class="n">optimize</span><span class="p">:</span>

    <span class="n">param_test0</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s1">&#39;n_estimators&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span><span class="mi">250</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">print</span> <span class="s1">&#39;performing hyperparamter optimization step 0&#39;</span>
    <span class="n">gsearch0</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb1</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test0</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">gsearch0</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">gsearch0</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">gsearch0</span><span class="o">.</span><span class="n">best_score_</span>

    <span class="n">param_test1</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s1">&#39;max_depth&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span>
     <span class="s1">&#39;min_child_weight&#39;</span><span class="p">:</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
    <span class="p">}</span>
    <span class="k">print</span> <span class="s1">&#39;performing hyperparamter optimization step 1&#39;</span>
    <span class="n">gsearch1</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">gsearch0</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span>
     <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test1</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">gsearch1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_score_</span>

    <span class="n">max_d</span> <span class="o">=</span> <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;max_depth&#39;</span><span class="p">]</span>
    <span class="n">min_c</span> <span class="o">=</span> <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;min_child_weight&#39;</span><span class="p">]</span>

    <span class="n">param_test2</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s1">&#39;gamma&#39;</span><span class="p">:[</span><span class="n">i</span><span class="o">/</span><span class="mf">10.</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)]</span>
    <span class="p">}</span>
    <span class="k">print</span> <span class="s1">&#39;performing hyperparamter optimization step 2&#39;</span>
    <span class="n">gsearch2</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> 
     <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test2</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">gsearch2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">gsearch2</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">gsearch2</span><span class="o">.</span><span class="n">best_score_</span>

    <span class="n">param_test3</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;subsample&#39;</span><span class="p">:[</span><span class="n">i</span><span class="o">/</span><span class="mf">10.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)],</span>
        <span class="s1">&#39;colsample_bytree&#39;</span><span class="p">:[</span><span class="n">i</span><span class="o">/</span><span class="mf">10.0</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)]</span>
    <span class="p">}</span>
    <span class="k">print</span> <span class="s1">&#39;performing hyperparamter optimization step 3&#39;</span>
    <span class="n">gsearch3</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">gsearch2</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> 
     <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">gsearch3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">gsearch3</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">gsearch3</span><span class="o">.</span><span class="n">best_score_</span>

    <span class="n">param_test4</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s1">&#39;reg_alpha&#39;</span><span class="p">:[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">10</span><span class="p">]</span>
    <span class="p">}</span>
    <span class="k">print</span> <span class="s1">&#39;performing hyperparamter optimization step 4&#39;</span>
    <span class="n">gsearch4</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">gsearch3</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> 
     <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test4</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">gsearch4</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="k">print</span> <span class="n">gsearch4</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">gsearch4</span><span class="o">.</span><span class="n">best_score_</span>

    <span class="n">alpha</span> <span class="o">=</span> <span class="n">gsearch4</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;reg_alpha&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">alpha</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">param_test4b</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s1">&#39;reg_alpha&#39;</span><span class="p">:[</span><span class="mf">0.1</span><span class="o">*</span><span class="n">alpha</span><span class="p">,</span> <span class="mf">0.25</span><span class="o">*</span><span class="n">alpha</span><span class="p">,</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">alpha</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="mf">2.5</span><span class="o">*</span><span class="n">alpha</span><span class="p">,</span> <span class="mi">5</span><span class="o">*</span><span class="n">alpha</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="n">alpha</span><span class="p">]</span>
        <span class="p">}</span>
        <span class="k">print</span> <span class="s1">&#39;performing hyperparamter optimization step 4b&#39;</span>
        <span class="n">gsearch4b</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">gsearch4</span><span class="o">.</span><span class="n">best_estimator_</span><span class="p">,</span> 
         <span class="n">param_grid</span> <span class="o">=</span> <span class="n">param_test4b</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span><span class="n">iid</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">gsearch4b</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
        <span class="k">print</span> <span class="n">gsearch4b</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">gsearch4</span><span class="o">.</span><span class="n">best_score_</span>
        <span class="k">print</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Parameter optimization finished!&#39;</span>
        <span class="n">xgb_opt</span> <span class="o">=</span> <span class="n">gsearch4b</span><span class="o">.</span><span class="n">best_estimator_</span>
        <span class="n">xgb_opt</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">xgb_opt</span> <span class="o">=</span> <span class="n">gsearch4</span><span class="o">.</span><span class="n">best_estimator_</span>
        <span class="n">xgb_opt</span>
<span class="k">else</span><span class="p">:</span> 
    <span class="c1"># Pre-optimized settings</span>
    <span class="n">xgb_opt</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">(</span><span class="n">base_score</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">colsample_bylevel</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
       <span class="n">gamma</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_delta_step</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
       <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">missing</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">70</span><span class="p">,</span> <span class="n">nthread</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
       <span class="n">objective</span><span class="o">=</span><span class="s1">&#39;binary:logistic&#39;</span><span class="p">,</span> <span class="n">reg_alpha</span><span class="o">=</span><span class="mf">25.0</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
       <span class="n">scale_pos_weight</span><span class="o">=</span><span class="mf">7.0909090909090908</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">silent</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
       <span class="n">subsample</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="k">print</span> <span class="n">xgb_opt</span>
</pre></div>


<div class="highlight"><pre><span></span>performing hyperparamter optimization step 0
{&#39;n_estimators&#39;: 110} 0.9749764110312029
performing hyperparamter optimization step 1
{&#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 1} 0.9802166944977622
performing hyperparamter optimization step 2
{&#39;gamma&#39;: 0.1} 0.980447807952243
performing hyperparamter optimization step 3
{&#39;subsample&#39;: 0.4, &#39;colsample_bytree&#39;: 0.9} 0.9815532764983201
performing hyperparamter optimization step 4
{&#39;reg_alpha&#39;: 0.001} 0.9818254583224896
performing hyperparamter optimization step 4b
{&#39;reg_alpha&#39;: 0.001} 0.9818254583224896

Parameter optimization finished!
XGBClassifier(base_score=0.5, booster=&#39;gbtree&#39;, colsample_bylevel=1,
       colsample_bytree=0.9, gamma=0.1, learning_rate=0.1,
       max_delta_step=0, max_depth=9, min_child_weight=1, missing=None,
       n_estimators=110, n_jobs=1, nthread=4, objective=&#39;binary:logistic&#39;,
       random_state=0, reg_alpha=0.001, reg_lambda=1, scale_pos_weight=1.0,
       seed=1, silent=True, subsample=0.4)
</pre></div>


<h1>K-fold cross-validation</h1>
<div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">eval_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">K</span><span class="p">))</span>
<span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>


<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span><span class="mi">7</span><span class="p">))</span>
<span class="n">mean_tpr</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">mean_fpr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">lw</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">i</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">roc_aucs_xgbopt</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_indices</span><span class="p">,</span> <span class="n">test_indices</span> <span class="ow">in</span> <span class="n">skf</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">train_indices</span><span class="p">]</span>
    <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">test_indices</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">test_indices</span><span class="p">]</span>
    <span class="n">class_weight_scale</span> <span class="o">=</span> <span class="mf">1.</span><span class="o">*</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">print</span> <span class="s1">&#39;class weight scale : {}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">class_weight_scale</span><span class="p">)</span>
    <span class="n">xgb_opt</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s1">&#39;scale_pos_weight&#39;</span> <span class="p">:</span> <span class="n">class_weight_scale</span><span class="p">})</span>
    <span class="n">xgb_opt</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">Y_train</span><span class="p">)</span>
    <span class="n">xgb_opt_pred_prob</span> <span class="o">=</span> <span class="n">xgb_opt</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_valid</span><span class="p">)</span>
    <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">y_valid</span><span class="p">,</span> <span class="n">xgb_opt_pred_prob</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">mean_tpr</span> <span class="o">+=</span> <span class="n">interp</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">mean_tpr</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">roc_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">)</span>
    <span class="n">roc_aucs_xgbopt</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">roc_auc</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC fold </span><span class="si">%d</span><span class="s1"> (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">roc_auc</span><span class="p">))</span>

    <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Luck&#39;</span><span class="p">)</span>

<span class="n">mean_tpr</span> <span class="o">/=</span> <span class="n">K</span>
<span class="n">mean_tpr</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.0</span>
<span class="n">mean_auc</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">mean_tpr</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_fpr</span><span class="p">,</span> <span class="n">mean_tpr</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Mean ROC (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">mean_auc</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="n">lw</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ROC curve&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>

<span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/ROC.png&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kr">class</span> <span class="nx">weight</span> <span class="nx">scale</span> : <span class="kt">1.0</span>
<span class="kr">class</span> <span class="nx">weight</span> <span class="nx">scale</span> : <span class="kt">1.0</span>
<span class="kr">class</span> <span class="nx">weight</span> <span class="nx">scale</span> : <span class="kt">1.0</span>
<span class="kr">class</span> <span class="nx">weight</span> <span class="nx">scale</span> : <span class="kt">1.0</span>
<span class="kr">class</span> <span class="nx">weight</span> <span class="nx">scale</span> : <span class="kt">1.0</span>
</pre></div>


<p><img alt="png" src="/images/output_80_1.png"></p>
<div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">optimize</span><span class="p">:</span>

    <span class="n">aucs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">roc_aucs_xgb1</span><span class="p">),</span>
            <span class="n">gsearch0</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span>
            <span class="n">gsearch1</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span>
            <span class="n">gsearch2</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span>
            <span class="n">gsearch3</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span>
            <span class="n">gsearch4</span><span class="o">.</span><span class="n">best_score_</span><span class="p">,</span>
            <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">roc_aucs_xgbopt</span><span class="p">)]</span>

    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">aucs</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">aucs</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">aucs</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">aucs</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">aucs</span><span class="p">)</span><span class="o">+</span><span class="mf">0.5</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.99</span><span class="o">*</span><span class="n">aucs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mf">1.01</span><span class="o">*</span><span class="n">aucs</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Hyperparamter optimization step&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;AUC&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Hyperparameter optimization&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/optimization.png&#39;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="/images/output_81_0.png"></p>
<h1>precision  recall  f1-score of testing set</h1>
<div class="highlight"><pre><span></span><span class="k">print</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">y_true</span> <span class="o">=</span> <span class="n">y_test</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred</span> <span class="o">=</span> <span class="n">xgb_opt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="o">.</span><span class="n">values</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span></span>              precision    recall  f1-score   support

           0       0.90      0.97      0.93      1605
           1       0.60      0.30      0.40       249

   micro avg       0.88      0.88      0.88      1854
   macro avg       0.75      0.64      0.67      1854
weighted avg       0.86      0.88      0.86      1854
</pre></div>


<h1>Features Importance</h1>
<div class="highlight"><pre><span></span><span class="n">xgb_opt</span><span class="o">.</span><span class="n">get_booster</span><span class="p">()</span><span class="o">.</span><span class="n">feature_names</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">my_plot_importance</span><span class="p">(</span><span class="n">booster</span><span class="p">,</span> <span class="n">figsize</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span> 
    <span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="kn">from</span> <span class="nn">xgboost</span> <span class="kn">import</span> <span class="n">plot_importance</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">figsize</span><span class="p">))</span>
    <span class="n">plot_importance</span><span class="p">(</span><span class="n">booster</span><span class="o">=</span><span class="n">booster</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="p">([</span><span class="n">ax</span><span class="o">.</span><span class="n">title</span><span class="p">,</span> <span class="n">ax</span><span class="o">.</span><span class="n">xaxis</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">ax</span><span class="o">.</span><span class="n">yaxis</span><span class="o">.</span><span class="n">label</span><span class="p">,]</span> <span class="o">+</span>
<span class="n">ax</span><span class="o">.</span><span class="n">get_xticklabels</span><span class="p">()</span> <span class="o">+</span> <span class="n">ax</span><span class="o">.</span><span class="n">get_yticklabels</span><span class="p">()):</span>
        <span class="n">item</span><span class="o">.</span><span class="n">set_fontsize</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;figures/Feature_importance.png&#39;</span><span class="p">)</span>

<span class="n">my_plot_importance</span><span class="p">(</span><span class="n">xgb_opt</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
</pre></div>


<p><img alt="png" src="/images/output_85_0.png"></p>
            <div class="hr"></div>
            <a href="#" class="go-top">Go Top</a>
<div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = "leafyleap-2"; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div><footer class="footer">
    <p>&copy; Mohcine Madkour &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
</div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>
</body>
</html>