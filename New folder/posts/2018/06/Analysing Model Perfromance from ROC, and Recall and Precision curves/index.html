<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Mohcine Madkour, Big Data Architectures and more">


        <title>Analysing Model Perfromance from Receiver Operator Characteristic and Recall and Precision curves // Mohcine Madkour // Big Data Architectures and more</title>


    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.3.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="../../../../theme/css/pure.css">
    <link rel="stylesheet" href="../../../../theme/css/pygments.css">

    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/fitvids/1.0.1/jquery.fitvids.min.js"></script>
    <script>
        $(document).ready(function(){
            $(".content").fitVids();
        });
    </script>
</head>

<body>
<div class="pure-g-r" id="layout">
    <div class="sidebar sidebar-article pure-u">
        <header class="header-article">
            <hgroup>
                <a href="../../../../author/mohcine-madkour.html" title="See posts by Mohcine Madkour">
                        <img class="avatar" alt="Mohcine Madkour" src="http://www.gravatar.com/avatar/ae08847efc1a85b710f326eb8ee2e907">
                </a>
                <h2 class="article-info">Mohcine Madkour</h2>
                <small class="about-author"></small>
                <h5>Published</h5>
                <p>Sat 09 June 2018</p>
                <a href="/">&larr;Home</a>
            </hgroup>
        </header>
    </div>
    <div class="pure-u">
        <div class="content">
            <section class="post">
                <header class="post-header">
                    <h1>Analysing Model Perfromance from Receiver Operator Characteristic and Recall and Precision curves</h1>
                        <p class="post-meta">
                            // under                                 <a class="post-category" href="../../../../tag/roc/">ROC</a>
                                <a class="post-category" href="../../../../tag/auroc/">AUROC</a>
                                <a class="post-category" href="../../../../tag/aucpr/">AUCPR</a>
                                <a class="post-category" href="../../../../tag/f1-score/">F1 Score</a>
                                <a class="post-category" href="../../../../tag/recall/">Recall</a>
                                <a class="post-category" href="../../../../tag/precision/">Precision</a>
                        </p>
                </header>
            </section>
            <p>ROC and PR curves are commonly used to present results for binary decision problems in machine learning. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. Each point of the ROC curve (i.e. threshold) corresponds to specific values of sensitivity and specificity. The area under the ROC curve (AUC) is a summary measure of performance that indicates whether on average a true positive is ranked higher than a false positives. If model A has higher AUC than model B, model A is performing better on average, but there still could be specific areas of the ROC space where model B is better (i.e. thresholds for which sensitivity and specificity are higher for model B than A. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. There is deep connection between ROC space and PR space, such that a curve dominates in ROC space if  and only if it dominates in PR space. The precision-recall curve shows the tradeoff between precision and recall for different threshold. A high area under the curve represents both high recall and high precision, where high precision, x-axis, relates to a low false positive rate, and high recall, y-axis, relates to a low false negative rate. High scores for both show that the classifier is returning accurate results (high precision), as well as returning a majority of all positive results (high recall).</p>
<h2>Sensitivity (positive in disease)</h2>
<p>Sensitivity is the ability of a test to correctly classify an individual as ′diseased′</p>
<div class="highlight"><pre><span></span>Sensitivity = a / a+c
= a (true positive) / a+c (true positive + false negative)
= Probability of being test positive when disease present.
</pre></div>


<h2>Specificity (negative in health)</h2>
<p>The ability of a test to correctly classify an individual as disease- free is called the test′s specificity</p>
<div class="highlight"><pre><span></span>Specificity = d / b+d
= d (true negative) / b+d (true negative + false positive)
= Probability of being test negative when disease absent.
</pre></div>


<p>Sensitivity and specificity are inversely proportional, meaning that as the sensitivity increases, the specificity decreases and vice versa.</p>
<h2>Positive Predictive Value (PPV)</h2>
<p>It is the percentage of patients with a positive test who actually have the disease. </p>
<div class="highlight"><pre><span></span>PPV: = a / a+b
= a (true positive) / a+b (true positive + false positive)
= Probability (patient having disease when test is positive)
</pre></div>


<h2>Negative Predictive Value (NPV)</h2>
<p>It is the percentage of patients with a negative test who do not have the disease.</p>
<div class="highlight"><pre><span></span>NPV:    =   d / c+d
=   d (true negative) / c+d (false negative + true negative)
=   Probability (patient not having disease when test is negative)
</pre></div>


<p>Positive and negative predictive values are influenced by the prevalence of disease in the population that is being tested. If we  test in a high prevalence setting, it is more likely that persons who test positive truly have disease than if the test is performed in a population with low prevalence. So the PPV will increase with increasing prevalence and NPV decreases with increase in prevalence.</p>
<h2>Methods to find the ‘optimal’ threshold point</h2>
<p>Three criteria  are  used to  find  optimal  threshold point  from  ROC  curve.  These criteria are known as points on curve closest to the (0, 1), Youden index, and minimize cost criterion. First two methods give equal weight to sensitivity and specificity and impose no ethical, cost, and no prevalence  constraints.  The  third  criterion  considers  cost  which  mainly  includes financial  cost  for  correct  and  false  diagnosis,  cost  of  discomfort  to  person  caused  by treatment, and cost of further investigation when needed.  This method is rarely used in medical literature because it is difficult to estimate the respective costs and prevalence is often difficult to assess.</p>
<p>Youden index  is  more  commonly  used  criterion  because  this  index  reflects  the  intension  to maximize the correct classification 
rate and is easy to calculate. It maximizes the vertical distance from line of equality to the point [x, y] as shown in Figure. The x represents (1-specificity) and y represents sensitivity.  In  other  words,  the  Youden  index  J  is  the  point on the ROC  curve  which  is farthest  from  line  of  equality  (diagonal  line).  The  main  aim of  Youden  index  is  to 
maximize the difference between TPR (sensitivity) and FPR (1 –specificty) and little algebra yields J = max[sensitivity+specificty].  The  value  of  J  for  continuous  test  can  be  located  by  doing  a  search  of plausible  values  where  sum  of  sensitivity  and  specificity  can be  maximum:</p>
<div class="highlight"><pre><span></span>j= model_metric[&#39;thres&#39;].iloc[model_metric[&#39;yod_index&#39;].idxmax()-1]
</pre></div>


<p>Sometimes a second cutoff that is bigger than j but less than 1 is needed. This cutoff can be used to stratify the positively predicted values to moderate and high prediction for example(needed for risk prediction stratification). This cutoff can be calculated using the accuracy measurement using the following method:</p>
<div class="highlight"><pre><span></span>cutoff2=cu.cal_cutoff2(model_metric)
</pre></div>


<p>with</p>
<div class="highlight"><pre><span></span>    def cal_cutoff2(data):
    val=0
    for i in range(len(data)-10):
    if((abs(data[&#39;acc&#39;].iloc[i]-data[&#39;acc&#39;].iloc[i+10]))&amp;lt;0.002):
        val=data[&#39;thres&#39;].iloc[i]
        break
return(val)
</pre></div>


<p>The chart bellow illustrates the relationship between the different performance metrics ( prevalence is exluded) in an example of 4 estimators apllied on 4 -classes data<img alt=" PPV and NPV  relationship" src="/images/NPV-PPV-Accracy-Youden.png"></p>
<h1>Calculation of performance metrics</h1>
<p>Here is the python code for the calculation of  performance metrics </p>
<div class="highlight"><pre><span></span>def calculate_metric(outcome, score):
    obser = np.zeros(len(outcome))
    obser[[i for i, x in enumerate(outcome) if x == 1 ]] = 1 ;
    obser = [float(i) for i in obser]
    score = [float(i) for i in score]
    prev = round(sum(obser)/len(obser),2)
    thres = np.arange(0.01,1.01,0.01)#(0.01,0.98,0.01)
    xval = thres
    acc = np.zeros(len(thres))
    ppv = np.zeros(len(thres))
    npv = np.zeros(len(thres))
    sen = np.zeros(len(thres))
    spe = np.zeros(len(thres))
    yod = np.zeros(len(thres))
    auc = np.zeros(len(thres))
    recall = np.zeros(len(thres))
    precision = np.zeros(len(thres))
    F1 = np.zeros(len(thres)) 
    for l in range(len(thres)):
        plotdata = ROC_parameters(obser,score,thres[l])
        acc[l] = round(plotdata[0],3)
        ppv[l] = round(plotdata[1],3)
        npv[l] = round(plotdata[2],3)
        sen[l] = round(plotdata[3],3)
        spe[l] = round(plotdata[4],3)
        yod[l] = round(plotdata[5],3)
        recall[l] = round(plotdata[6],3)
        precision[l] = round(plotdata[7],3)
        F1[l] = round(plotdata[8],3)
        auc[l] = roc_auc_score(obser, score)
    prev = round(sum(obser)/len(obser),2)
    #roc_vals=np.zeros((length(spe),8))
    roc_vals=pd.DataFrame(index=range(1,101), columns=[[&quot;thres&quot;,&quot;acc&quot;,&quot;ppv&quot;,&quot;npv&quot;,&quot;specificity&quot;,&quot;sensitivity&quot;,&quot;yod_index&quot;,&quot;recall&quot;,&quot;precision&quot;,&quot;F1&quot;,&quot;auc&quot;]])
    #roc_vals &amp;lt;- dacolnames(roc_vals) &amp;lt;- c(&quot;thres&quot;,&quot;acc&quot;,&quot;ppv&quot;,&quot;npv&quot;,&quot;specificity&quot;,&quot;sensitivity&quot;,&quot;yod_index&quot;,&quot;auc&quot;)
    roc_vals[&#39;thres&#39;]=thres
    roc_vals[&#39;acc&#39;]= acc
    roc_vals[&#39;ppv&#39;] = ppv
    roc_vals[&#39;npv&#39;] = npv
    roc_vals[&#39;specificity&#39;] =spe
    roc_vals[&#39;sensitivity&#39;] = sen
    roc_vals[&#39;yod_index&#39;] = yod;
    roc_vals[&#39;recall&#39;] = recall;
    roc_vals[&#39;precision&#39;] = precision
    roc_vals[&#39;F1&#39;] = F1;
    roc_vals[&#39;auc&#39;] = auc;
    return roc_vals

def ROC_parameters(obser,score,thr):
    #print obser,score,thr
    temp=np.zeros(len(score))
    #print thr;
    temp[[ i for i, x in enumerate(score) if x &amp;gt;= thr ]]= 1
    p_ind=[ i for i, x in enumerate(obser) if x == 1 ]
    n_ind = [ i for i, x in enumerate(obser) if x == 0 ]
    TP = sum(temp[p_ind]==1)
    FP = sum(temp[n_ind]==1)
    TN =sum(temp[n_ind]==0)
    FN = sum(temp[p_ind]==0)
    acc = (float)(TP+TN)/len(temp)
    recall=0
    precision=0
    #print TP,FP,TN,FN;
    if TP+FP&amp;gt;0:
        ppv = (float)(TP)/(TP+FP)
    else:
        ppv=np.NaN
    if TN+FN&amp;gt;0:
        npv = (float)(TN)/(TN+FN)
    else:
        npv=np.NaN
    if TP+FN&amp;gt;0:
        sen = (float)(TP)/(TP+FN)
    else:
        sen=np.NaN
    if TN+FP&amp;gt;0:
        spe = (float)(TN)/(TN+FP)
    else:
        spe=np.NaN
    if TP+FN&amp;gt;0:
        recall = (float)(TP)/(TP+FN)
    else:
        recall=np.NaN    
    if TP+FP&amp;gt;0:
        precision = (float)(TP)/(TP+FP)
    else:
        precision=np.NaN
    if recall+precision&amp;gt;0:         
        F1 = (float)((2*recall*precision)/(recall+precision))
    else:
        F1=np.NaN
    yod = (float)(sen+spe-1)
    ls=list();
    ls.append(acc)
    ls.append(ppv)
    ls.append(npv)
    ls.append(sen)
    ls.append(spe)
    ls.append(yod)
    ls.append(recall)
    ls.append(precision)
    ls.append(F1)
    return ls
</pre></div>


<h1>calculating performance measurements and confidence intervals using Boostraping</h1>
<div class="highlight"><pre><span></span>def calculate_metric_boostrap(outcome, score):
    d = []
    for p in range(0,len(score)):
        d.append((score[p]))
    score=pd.Series(d)
    n_bootstraps = 0
    rng_seed = 42  # control reproducibility
    scores_table = {} 
    rng = np.random.RandomState(rng_seed)
    for i in range(n_bootstraps):
    # bootstrap by sampling with replacement on the prediction indices
        indices = rng.random_integers(0, len(outcome) - 1, len(outcome))
        if len(np.unique(outcome[indices])) &amp;lt; 2:
        # We need at least one positive and one negative sample for ROC AUC
        # to be defined: reject the sample
            continue
        scores_table[i]= calculate_metric(outcome[indices], score[indices])

    panel = pd.Panel(scores_table)
    df=panel.mean(axis=0)
    return df,panel

def confidence_interval(panel):
    vector = []
    confidence_lower=panel[1].copy()
    confidence_upper=panel[1].copy()
    nr=len(panel[1].axes[0])
    nc=len(panel[1].axes[1])
    for ix in  range(0,nr):
        for iy in range(0,nc):
            vector = []
            for k, df in panel.iteritems():
                vector.append(df.iloc[ix,iy])
            sorted_vector = np.array(vector)
            sorted_vector.sort()
            confidence_lower.iloc[ix,iy] = sorted_vector[int(0.05 * len(sorted_vector))]
            confidence_upper.iloc[ix,iy] = sorted_vector[int(0.95 * len(sorted_vector))]
    return confidence_lower, confidence_upper
</pre></div>


<p>The chart bellow illustrates ROC and PR curves for 4 different estimators applied on the same data<img alt=" ROC" src="/images/ROC-PV1.png"><img alt=" PR" src="/images/ROC-PV2.png">
The performances of the algorithms appear to be comparable in ROC space, however, in PR space we can see that Estimator 4 has a clear advantage over Estimator 3.</p>
<h1>Assessing the Model</h1>
<p>The chart bellow outlines the performance evaluation pipeline (except prevalence) in an example<img alt=" performance evaluation pipeline" src="/images/MLPipe.jpg"></p>
<h1>The bias-variance trade-off (Bias-variance dilemma)</h1>
<p>Bias and variance are inherent properties of estimators and we usually have to select learning algorithms and hyperparameters so that both bias and variance are as low as possible. Another way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.
<img alt=" bias-variance trade-off" src="/images/biasvariance.png"></p>
<h1>Analyzing Model Variance and Bias</h1>
<p>The two methods used here for analyzing how the model is performing with the data are Learning Curves and a Model Complexity plot.Learning curves give us an opportunity to diagnose bias and variance in supervised learning models. </p>
<h2>Learning Curves</h2>
<p>A learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a <strong>variance error</strong> or a <strong>bias error</strong>. If both the validation score and the training score converge to a value that is <strong>too low</strong> with increasing size of the training set, we will not benefit much from more training data. In the following plot you can see an example: naive Bayes roughly converges to a low score.
We will probably have to <strong>use an estimator</strong> or a <strong>parametrization of the current estimator</strong> that can learn more <strong>complex concepts</strong> (i.e. has a lower bias). If the training score is much greater than the validation score for the maximum number of training samples (<strong>i.e. has a high variance</strong>), adding more training samples will most likely increase generalization. In the following plot you can see that the SVM could benefit from more training examples.</p>
<h1>Example of Scoring Learners and Cohort</h1>
<table>
<thead>
<tr>
<th>Cohort Definition</th>
<th>Cohort Size</th>
<th>CVD Percent in Cohort</th>
<th>Covariates in Learner/Model</th>
<th>Method Type</th>
<th>Method Sensitivity</th>
<th>Method PPV</th>
<th>Balanced Accuracy</th>
<th>Method Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td>ALL OF THEM (don't emulate</td>
<td>369000</td>
<td>0.80%</td>
<td>"bmi</td>
<td>numAge</td>
<td>tchol</td>
<td>sbp</td>
<td>htn</td>
<td>t2d"</td>
</tr>
<tr>
<td>Age &gt; 55</td>
<td>122792</td>
<td>22.60%</td>
<td>"numAge</td>
<td>tchol</td>
<td>htn</td>
<td>gender"</td>
<td>Logit</td>
<td>0.16</td>
</tr>
<tr>
<td>Age 20-40</td>
<td>121130</td>
<td>0.02%</td>
<td>"tchol</td>
<td>t2d</td>
<td>smoking</td>
<td>race"</td>
<td>LDA</td>
<td>0</td>
</tr>
<tr>
<td>"htn == ""Y"""</td>
<td>108510</td>
<td>18.85%</td>
<td>smoking</td>
<td>Logit</td>
<td>"""NA"""</td>
<td>"""NA"""</td>
<td>"""NA"" (is this weird?)"</td>
<td></td>
</tr>
<tr>
<td>"gender == ""F"" &amp; numAge &gt; 60"</td>
<td>53929</td>
<td>14.30%</td>
<td>"tchol</td>
<td>htn"</td>
<td>Logit</td>
<td>0</td>
<td>NaN</td>
<td>0.5</td>
</tr>
<tr>
<td>Age 30-45</td>
<td>99930</td>
<td>"numAge</td>
<td>race</td>
<td>htn</td>
<td>gender</td>
<td>smoking"</td>
<td>Logit</td>
<td>1</td>
</tr>
<tr>
<td>Age &lt;= 40</td>
<td>93980 train; 93981 test</td>
<td>1.64%</td>
<td>"numAge</td>
<td>htn</td>
<td>smoking</td>
<td>treat</td>
<td>t2d</td>
<td>gender</td>
</tr>
<tr>
<td>"gender == ""M"" &amp; numAge &gt; 60"</td>
<td>36853</td>
<td>30.75%</td>
<td>"tchol</td>
<td>htn"</td>
<td>Logit</td>
<td>0.28</td>
<td>0.51</td>
<td>0.58</td>
</tr>
<tr>
<td>GENETICS</td>
<td>66100</td>
<td>2.40%</td>
<td>"tchol</td>
<td>rs8055236</td>
<td>htn</td>
<td>t2d</td>
<td>smoking"</td>
<td>lda</td>
</tr>
<tr>
<td>age &lt; 55 &amp; age &gt; 35</td>
<td>379272</td>
<td>5.30%</td>
<td>cvd ~ tchol + htn + t2d + bmi + rs8055236</td>
<td>logit</td>
<td>0</td>
<td>nan</td>
<td>0.5</td>
<td></td>
</tr>
<tr>
<td>age&gt;55</td>
<td>logit</td>
<td>0.22</td>
<td>0.59</td>
<td>0.6</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>GENETICS</td>
<td>46217 train; 8157 test</td>
<td>9.78%</td>
<td>"cvd ~ numAge + htn + smoking</td>
<td>+ treat + t2d + gender + bmi + tchol + sbp + rs10757278 + rs4665058 + rs8055236"</td>
<td>SuperLearner</td>
<td>0.9</td>
<td>0.369</td>
<td>0.8394</td>
</tr>
</tbody>
</table>
            <div class="hr"></div>
            <a href="#" class="go-top">Go Top</a>
<div class="comments">
    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = "leafyleap-2"; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div><footer class="footer">
    <p>&copy; Mohcine Madkour &ndash;
        Built with <a href="https://github.com/PurePelicanTheme/pure">Pure Theme</a>
        for <a href="http://blog.getpelican.com/">Pelican</a>
    </p>
</footer>        </div>
    </div>
</div>
    <script>
        var $top = $('.go-top');

        // Show or hide the sticky footer button
        $(window).scroll(function() {
            if ($(this).scrollTop() > 200) {
                $top.fadeIn(200);
            } else {
                $top.fadeOut(200);
            }
        });

        // Animate the scroll to top
        $top.click(function(event) {
            event.preventDefault();
            $('html, body').animate({scrollTop: 0}, 300);
        })

        // Makes sure that the href="#" attached to the <a> elements
        // don't scroll you back up the page.
        $('body').on('click', 'a[href="#"]', function(event) {
            event.preventDefault();
        });
    </script>
</body>
</html>